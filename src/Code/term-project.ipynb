{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664849 601691\n",
      "precision 0.49748743718592964\n",
      "accuracy 0.5\n",
      "recall 1.0\n",
      "F_measure 0.6644295302013422\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "#I use unigram to implement the baseline\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import movie_reviews # use the built-in data in ntlk.corpus\n",
    "\n",
    "data = []\n",
    "for file in movie_reviews.fileids():\n",
    "    for file_type in movie_reviews.categories(file):\n",
    "        word_list = []\n",
    "        for word in movie_reviews.words(file):\n",
    "            #preprocessing to leave out punctuation marks\n",
    "            word_list.append(word)\n",
    "        data.append((file_type, word_list))\n",
    "\n",
    "\n",
    "random.shuffle(data) # This is to let our data shuffle so that there can be train and test data generated from it (pos and neg are not seperated)\n",
    "# print(len(data))\n",
    "train = data[0:1600]\n",
    "test = data[1600:2000]\n",
    "#4:1 train:test data\n",
    "\n",
    "\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "for file_type, words in train:\n",
    "    for word in words:\n",
    "        if word in pos and file_type == \"pos\":\n",
    "            pos[word] += 1\n",
    "        if word not in pos and file_type == \"pos\":\n",
    "            pos[word] = 1\n",
    "        if word in neg and file_type == \"neg\":\n",
    "            neg[word] += 1\n",
    "        if word not in neg and file_type == \"neg\":\n",
    "            neg[word] = 1\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "sp = 0\n",
    "sn = 0\n",
    "for value in pos.values():\n",
    "    sp += value\n",
    "for value in neg.values():\n",
    "    sn += value\n",
    "print(sp, sn)\n",
    "for file_type, words in test:\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for word in words:\n",
    "        if word not in pos.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_pos += pos.get(word) / sp\n",
    "        if word not in neg.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_neg += neg.get(word) / sn \n",
    "        \n",
    "#     print(count_pos, count_neg)\n",
    "    if (count_pos > count_neg) and (file_type == 'pos'):\n",
    "        tp += 1\n",
    "    if (count_pos > count_neg) and (file_type == 'neg'):\n",
    "        fp += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'pos'):\n",
    "        fn += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'neg'):\n",
    "        tn += 1\n",
    "print('precision', tp/ (tp + fp))\n",
    "print('accuracy', (tp+tn)/len(test))\n",
    "print('recall', tp/ (tp + fn))\n",
    "print('F_measure', 2 * (tp/ (tp + fp) * tp/ (tp + fn))/(tp/ (tp + fp) + tp/ (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568755 516069\n",
      "precision 0.5075\n",
      "accuracy 0.5075\n",
      "recall 1.0\n",
      "F_measure 0.6733001658374793\n"
     ]
    }
   ],
   "source": [
    "#improve baseline by removing punctuation marks\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import movie_reviews # use the built-in data in ntlk.corpus\n",
    "\n",
    "data = []\n",
    "for file in movie_reviews.fileids():\n",
    "    for file_type in movie_reviews.categories(file):\n",
    "        word_list = []\n",
    "        for word in movie_reviews.words(file):\n",
    "            #preprocessing to leave out punctuation marks\n",
    "            if word not in [\"&\", \",\", \".\", \"?\", \"/\", \"'\", \"(\", \")\", \"<\", \">\", \"!\", \"-\"]:\n",
    "                word_list.append(word)\n",
    "        data.append((file_type, word_list))\n",
    "\n",
    "\n",
    "random.shuffle(data) # This is to let our data shuttfle so that there can be train and test data generated from it (pos and neg are not seperated)\n",
    "# print(len(data))\n",
    "train = data[0:1600]\n",
    "test = data[1600:2000]\n",
    "\n",
    "\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "for file_type, words in train:\n",
    "    for word in words:\n",
    "        if word in pos and file_type == \"pos\":\n",
    "            pos[word] += 1\n",
    "        if word not in pos and file_type == \"pos\":\n",
    "            pos[word] = 1\n",
    "        if word in neg and file_type == \"neg\":\n",
    "            neg[word] += 1\n",
    "        if word not in neg and file_type == \"neg\":\n",
    "            neg[word] = 1\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "sp = 0\n",
    "sn = 0\n",
    "for value in pos.values():\n",
    "    sp += value\n",
    "for value in neg.values():\n",
    "    sn += value\n",
    "print(sp, sn)\n",
    "for file_type, words in test:\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for word in words:\n",
    "        if word not in pos.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_pos += pos.get(word) / sp\n",
    "        if word not in neg.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_neg += neg.get(word) / sn \n",
    "        \n",
    "#     print(count_pos, count_neg)\n",
    "    if (count_pos > count_neg) and (file_type == 'pos'):\n",
    "        tp += 1\n",
    "    if (count_pos > count_neg) and (file_type == 'neg'):\n",
    "        fp += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'pos'):\n",
    "        fn += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'neg'):\n",
    "        tn += 1\n",
    "print('precision', tp/ (tp + fp))\n",
    "print('accuracy', (tp+tn)/len(test))\n",
    "print('recall', tp/ (tp + fn))\n",
    "print('F_measure', 2 * (tp/ (tp + fp) * tp/ (tp + fn))/(tp/ (tp + fp) + tp/ (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678162 594513\n",
      "precision 0.4824120603015075\n",
      "accuracy 0.4825\n",
      "recall 0.9948186528497409\n",
      "F_measure 0.6497461928934011\n"
     ]
    }
   ],
   "source": [
    "#improve baseline by making all words lower-case\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import movie_reviews # use the built-in data in ntlk.corpus\n",
    "\n",
    "data = []\n",
    "for file in movie_reviews.fileids():\n",
    "    for file_type in movie_reviews.categories(file):\n",
    "        word_list = []\n",
    "        for word in movie_reviews.words(file):\n",
    "            #preprocessing to leave out punctuation marks\n",
    "            word_list.append(word.lower())\n",
    "        data.append((file_type, word_list))\n",
    "\n",
    "\n",
    "random.shuffle(data) # This is to let our data shuttfle so that there can be train and test data generated from it (pos and neg are not seperated)\n",
    "# print(len(data))\n",
    "train = data[0:1600]\n",
    "test = data[1600:2000]\n",
    "\n",
    "\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "for file_type, words in train:\n",
    "    for word in words:\n",
    "        if word in pos and file_type == \"pos\":\n",
    "            pos[word] += 1\n",
    "        if word not in pos and file_type == \"pos\":\n",
    "            pos[word] = 1\n",
    "        if word in neg and file_type == \"neg\":\n",
    "            neg[word] += 1\n",
    "        if word not in neg and file_type == \"neg\":\n",
    "            neg[word] = 1\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "sp = 0\n",
    "sn = 0\n",
    "for value in pos.values():\n",
    "    sp += value\n",
    "for value in neg.values():\n",
    "    sn += value\n",
    "print(sp, sn)\n",
    "for file_type, words in test:\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for word in words:\n",
    "        if word not in pos.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_pos += pos.get(word) / sp\n",
    "        if word not in neg.keys():\n",
    "            pass\n",
    "        else:\n",
    "            count_neg += (neg.get(word)+1) / sn \n",
    "        \n",
    "#     print(count_pos, count_neg)\n",
    "    if (count_pos > count_neg) and (file_type == 'pos'):\n",
    "        tp += 1\n",
    "    if (count_pos > count_neg) and (file_type == 'neg'):\n",
    "        fp += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'pos'):\n",
    "        fn += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'neg'):\n",
    "        tn += 1\n",
    "print('precision', tp/ (tp + fp))\n",
    "print('accuracy', (tp+tn)/len(test))\n",
    "print('recall', tp/ (tp + fn))\n",
    "print('F_measure', 2 * (tp/ (tp + fp) * tp/ (tp + fn))/(tp/ (tp + fp) + tp/ (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662512 595883\n",
      "precision 0.485\n",
      "accuracy 0.485\n",
      "recall 1.0\n",
      "F_measure 0.6531986531986533\n"
     ]
    }
   ],
   "source": [
    "#improve baseline by adding one smoothing\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import movie_reviews # use the built-in data in ntlk.corpus\n",
    "\n",
    "data = []\n",
    "for file in movie_reviews.fileids():\n",
    "    for file_type in movie_reviews.categories(file):\n",
    "        word_list = []\n",
    "        for word in movie_reviews.words(file):\n",
    "            word_list.append(word)\n",
    "        data.append((file_type, word_list))\n",
    "\n",
    "random.shuffle(data) # This is to let our data shuttfle so that there can be train and test data generated from it (pos and neg are not seperated)\n",
    "# print(len(data))\n",
    "train = data[0:1600]\n",
    "test = data[1600:2000]\n",
    "#4:1 train:test data\n",
    "\n",
    "\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "for file_type, words in train:\n",
    "    for word in words:\n",
    "        if word in pos and file_type == \"pos\":\n",
    "            pos[word] += 1\n",
    "        if word not in pos and file_type == \"pos\":\n",
    "            pos[word] = 1\n",
    "        if word in neg and file_type == \"neg\":\n",
    "            neg[word] += 1\n",
    "        if word not in neg and file_type == \"neg\":\n",
    "            neg[word] = 1\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "sp = 0\n",
    "sn = 0\n",
    "for value in pos.values():\n",
    "    sp += value\n",
    "for value in neg.values():\n",
    "    sn += value\n",
    "print(sp, sn)\n",
    "for file_type, words in test:\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for word in words:\n",
    "        if word not in pos.keys():\n",
    "            count_pos += 1/(sp + len(pos))\n",
    "        else:\n",
    "            count_pos += (pos.get(word) + 1) / (sp + len(pos))\n",
    "        if word not in neg.keys():\n",
    "            count_neg += 1/(sn + len(neg))\n",
    "        else:\n",
    "            count_neg += (neg.get(word) + 1) / (sn + len(neg))\n",
    "        \n",
    "#     print(count_pos, count_neg)\n",
    "    if (count_pos > count_neg) and (file_type == 'pos'):\n",
    "        tp += 1\n",
    "    if (count_pos > count_neg) and (file_type == 'neg'):\n",
    "        fp += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'pos'):\n",
    "        fn += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'neg'):\n",
    "        tn += 1\n",
    "print('precision', tp/ (tp + fp))\n",
    "print('accuracy', (tp+tn)/len(test))\n",
    "print('recall', tp/ (tp + fn))\n",
    "print('F_measure', 2 * (tp/ (tp + fp) * tp/ (tp + fn))/(tp/ (tp + fp) + tp/ (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568208 509695\n",
      "precision 0.49874686716791977\n",
      "accuracy 0.4975\n",
      "recall 0.995\n",
      "F_measure 0.664440734557596\n"
     ]
    }
   ],
   "source": [
    "#improve baseline by adding one smoothing, change words to lower case and remove punctuation\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import movie_reviews # use the built-in data in ntlk.corpus\n",
    "\n",
    "data = []\n",
    "for file in movie_reviews.fileids():\n",
    "    for file_type in movie_reviews.categories(file):\n",
    "        word_list = []\n",
    "        for word in movie_reviews.words(file):\n",
    "            if word not in [\"&\", \",\", \".\", \"?\", \"/\", \"'\", \"(\", \")\", \"<\", \">\", \"!\", \"-\"]:\n",
    "                word_list.append(word.lower())\n",
    "        data.append((file_type, word_list))\n",
    "\n",
    "random.shuffle(data) # This is to let our data shuttfle so that there can be train and test data generated from it (pos and neg are not seperated)\n",
    "# print(len(data))\n",
    "train = data[0:1600]\n",
    "test = data[1600:2000]\n",
    "#4:1 train:test data\n",
    "\n",
    "\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "for file_type, words in train:\n",
    "    for word in words:\n",
    "        if word in pos and file_type == \"pos\":\n",
    "            pos[word] += 1\n",
    "        if word not in pos and file_type == \"pos\":\n",
    "            pos[word] = 1\n",
    "        if word in neg and file_type == \"neg\":\n",
    "            neg[word] += 1\n",
    "        if word not in neg and file_type == \"neg\":\n",
    "            neg[word] = 1\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "sp = 0\n",
    "sn = 0\n",
    "for value in pos.values():\n",
    "    sp += value\n",
    "for value in neg.values():\n",
    "    sn += value\n",
    "print(sp, sn)\n",
    "for file_type, words in test:\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for word in words:\n",
    "        if word not in pos.keys():\n",
    "            count_pos += 1/(sp + len(pos))\n",
    "        else:\n",
    "            count_pos += (pos.get(word) + 1) / (sp + len(pos))\n",
    "        if word not in neg.keys():\n",
    "            count_neg += 1/(sn + len(neg))\n",
    "        else:\n",
    "            count_neg += (neg.get(word) + 1) / (sn + len(neg))\n",
    "        \n",
    "#     print(count_pos, count_neg)\n",
    "    if (count_pos > count_neg) and (file_type == 'pos'):\n",
    "        tp += 1\n",
    "    if (count_pos > count_neg) and (file_type == 'neg'):\n",
    "        fp += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'pos'):\n",
    "        fn += 1\n",
    "    if (count_pos <= count_neg) and (file_type == 'neg'):\n",
    "        tn += 1\n",
    "print('precision', tp/ (tp + fp))\n",
    "print('accuracy', (tp+tn)/len(test))\n",
    "print('recall', tp/ (tp + fn))\n",
    "print('F_measure', 2 * (tp/ (tp + fp) * tp/ (tp + fn))/(tp/ (tp + fp) + tp/ (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes\n",
      "precision:  0.604904871949307\n",
      "recall:  0.6070494748910097\n",
      "accuracy:  0.6063621287501885\n",
      "F_measure:  0.6023851131020319\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "precision:  0.9658004158004159\n",
      "recall:  0.9660470248705544\n",
      "accuracy:  0.9659279360771897\n",
      "F_measure:  0.9658490116135766\n",
      "\n",
      "\n",
      "SVM\n",
      "precision:  0.9668735064349099\n",
      "recall:  0.9677758446320139\n",
      "accuracy:  0.9674280114578622\n",
      "F_measure:  0.9671834968976276\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#machine learning models\n",
    "#we try to build classfier based on whether it is pos or not.\n",
    "#1 stands for pos, 0 stands for neg\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "#referenced from https://blog.csdn.net/Yumi_huang/article/details/80015043\n",
    "\n",
    "word_dic = {}\n",
    "classes = []\n",
    "count = 0\n",
    "for word in set(movie_reviews.words()):\n",
    "    word_dic[word] = count\n",
    "    count += 1\n",
    "#use the same preprocessed data, but add the classifier\n",
    "x_raws = []\n",
    "for file_type, words in data:\n",
    "    one_file = []\n",
    "    if file_type == 'pos':\n",
    "        classes.append(1)\n",
    "    if file_type == 'neg':\n",
    "        classes.append(0)\n",
    "    count = 0\n",
    "    # This feature selection is referenced from Hongshuo's idea\n",
    "    for word in words:\n",
    "        if count == 90: # choose only 90 features (the first 90 elements)\n",
    "            break\n",
    "        one_file.append(word_dic[word])\n",
    "        count += 1\n",
    "    x_raws.append(one_file)\n",
    "y = np.array(classes)\n",
    "X = csr_matrix(pd.DataFrame(x_raws).dropna())       \n",
    "\n",
    "# The following code is directly borrowed from my assignment3\n",
    "count = 0\n",
    "for classifier_name in [\"Naïve Bayes\", \"Decision Tree\", \"SVM\"]:\n",
    "    classifier_method = None\n",
    "    if classifier_name == \"Naïve Bayes\":\n",
    "        classifier_method = GaussianNB()\n",
    "    elif classifier_name == \"Decision Tree\":\n",
    "        classifier_method = DecisionTreeClassifier()\n",
    "    else:\n",
    "        classifier_method = SVC(C=2.0, probability=True)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    F_measure = []\n",
    "    for tr_idx, te_idx in KFold(30).split(X):\n",
    "        ytr = y[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        yte = y[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        Xtr=X[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        Xte=X[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        # fit, predict and then get precision, recall, accuracy, F_measure\n",
    "        classifier_method.fit(Xtr.todense(), ytr)\n",
    "        ypre = classifier_method.predict(Xte.todense())\n",
    "        single_precision = precision_score(yte,ypre, average=\"macro\")\n",
    "        single_recall = recall_score(yte,ypre, average=\"macro\")\n",
    "        single_accuracy = accuracy_score(yte,ypre)\n",
    "        single_F_measure = f1_score(yte,ypre, average=\"macro\")\n",
    "        precision.append(single_precision)\n",
    "        recall.append(single_recall)\n",
    "        accuracy.append(single_accuracy)\n",
    "        F_measure.append(single_F_measure)\n",
    "    if count == 0:\n",
    "        print(\"Naïve Bayes\")\n",
    "    elif count == 1:\n",
    "        print(\"Decision Tree\")\n",
    "    else:\n",
    "        print(\"SVM\")\n",
    "    print(\"precision: \", sum(precision)/len(precision))\n",
    "    print(\"recall: \", sum(recall)/len(recall))\n",
    "    print(\"accuracy: \", sum(accuracy)/len(accuracy))\n",
    "    print(\"F_measure: \", sum(F_measure)/len(F_measure))\n",
    "    print(\"\\n\")\n",
    "    count += 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes\n",
      "precision:  0.5293422087771963\n",
      "recall:  0.5294172936736425\n",
      "accuracy:  0.5318709482888587\n",
      "F_measure:  0.5258812360379529\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "precision:  0.9613308544858987\n",
      "recall:  0.9612817581650173\n",
      "accuracy:  0.9613900195989747\n",
      "F_measure:  0.961115642583954\n",
      "\n",
      "\n",
      "SVM\n",
      "precision:  0.6982029333178017\n",
      "recall:  0.6986203769552258\n",
      "accuracy:  0.7003769033619782\n",
      "F_measure:  0.6957112458046608\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#machine learning models\n",
    "#we try to build classfier based on whether it is pos or not.\n",
    "#1 stands for pos, 0 stands for neg\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "#referenced from https://blog.csdn.net/Yumi_huang/article/details/80015043\n",
    "\n",
    "word_dic = {}\n",
    "classes = []\n",
    "count = 0\n",
    "for word in set(movie_reviews.words()):\n",
    "    word_dic[word] = count\n",
    "    count += 1\n",
    "#use the same preprocessed data, but add the classifier\n",
    "x_raws = []\n",
    "for file_type, words in data:\n",
    "    one_file = []\n",
    "    if file_type == 'pos':\n",
    "        classes.append(1)\n",
    "    if file_type == 'neg':\n",
    "        classes.append(0)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if count == 10: # choose only 10 features (the first 10 elements)\n",
    "            break\n",
    "        one_file.append(word_dic[word])\n",
    "        count += 1\n",
    "    x_raws.append(one_file)\n",
    "y = np.array(classes)\n",
    "X = csr_matrix(pd.DataFrame(x_raws).dropna())       \n",
    "\n",
    "# The following code is directly borrowed from my assignment3\n",
    "count = 0\n",
    "for classifier_name in [\"Naïve Bayes\", \"Decision Tree\", \"SVM\"]:\n",
    "    classifier_method = None\n",
    "    if classifier_name == \"Naïve Bayes\":\n",
    "        classifier_method = GaussianNB()\n",
    "    elif classifier_name == \"Decision Tree\":\n",
    "        classifier_method = DecisionTreeClassifier()\n",
    "    else:\n",
    "        classifier_method = SVC(C=2.0, probability=True)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    F_measure = []\n",
    "    for tr_idx, te_idx in KFold(30).split(X):\n",
    "        ytr = y[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        yte = y[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        Xtr=X[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        Xte=X[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        # fit, predict and then get precision, recall, accuracy, F_measure\n",
    "        classifier_method.fit(Xtr.todense(), ytr)\n",
    "        ypre = classifier_method.predict(Xte.todense())\n",
    "        single_precision = precision_score(yte,ypre, average=\"macro\")\n",
    "        single_recall = recall_score(yte,ypre, average=\"macro\")\n",
    "        single_accuracy = accuracy_score(yte,ypre)\n",
    "        single_F_measure = f1_score(yte,ypre, average=\"macro\")\n",
    "        precision.append(single_precision)\n",
    "        recall.append(single_recall)\n",
    "        accuracy.append(single_accuracy)\n",
    "        F_measure.append(single_F_measure)\n",
    "    if count == 0:\n",
    "        print(\"Naïve Bayes\")\n",
    "    elif count == 1:\n",
    "        print(\"Decision Tree\")\n",
    "    else:\n",
    "        print(\"SVM\")\n",
    "    print(\"precision: \", sum(precision)/len(precision))\n",
    "    print(\"recall: \", sum(recall)/len(recall))\n",
    "    print(\"accuracy: \", sum(accuracy)/len(accuracy))\n",
    "    print(\"F_measure: \", sum(F_measure)/len(F_measure))\n",
    "    print(\"\\n\")\n",
    "    count += 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes\n",
      "precision:  0.6345053893153507\n",
      "recall:  0.6352333745759787\n",
      "accuracy:  0.6343283582089553\n",
      "F_measure:  0.6305392057875256\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "precision:  0.9622816609713162\n",
      "recall:  0.962301328658065\n",
      "accuracy:  0.9623699683401177\n",
      "F_measure:  0.9619526696420125\n",
      "\n",
      "\n",
      "SVM\n",
      "precision:  0.9628936788679436\n",
      "recall:  0.9628781458952237\n",
      "accuracy:  0.9628976330468867\n",
      "F_measure:  0.9628454367265794\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#machine learning models\n",
    "#we try to build classfier based on whether it is pos or not.\n",
    "#1 stands for pos, 0 stands for neg\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "#referenced from https://blog.csdn.net/Yumi_huang/article/details/80015043\n",
    "\n",
    "word_dic = {}\n",
    "classes = []\n",
    "count = 0\n",
    "for word in set(movie_reviews.words()):\n",
    "    word_dic[word] = count\n",
    "    count += 1\n",
    "#use the same preprocessed data, but add the classifier\n",
    "x_raws = []\n",
    "for file_type, words in data:\n",
    "    one_file = []\n",
    "    if file_type == 'pos':\n",
    "        classes.append(1)\n",
    "    if file_type == 'neg':\n",
    "        classes.append(0)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if count == 150: # choose only 150 features (the first 150 elements)\n",
    "            break\n",
    "        one_file.append(word_dic[word])\n",
    "        count += 1\n",
    "    x_raws.append(one_file)\n",
    "y = np.array(classes)\n",
    "X = csr_matrix(pd.DataFrame(x_raws).dropna())       \n",
    "\n",
    "# The following code is directly borrowed from my assignment3\n",
    "count = 0\n",
    "for classifier_name in [\"Naïve Bayes\", \"Decision Tree\", \"SVM\"]:\n",
    "    classifier_method = None\n",
    "    if classifier_name == \"Naïve Bayes\":\n",
    "        classifier_method = GaussianNB()\n",
    "    elif classifier_name == \"Decision Tree\":\n",
    "        classifier_method = DecisionTreeClassifier()\n",
    "    else:\n",
    "        classifier_method = SVC(C=2.0, probability=True)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    F_measure = []\n",
    "    for tr_idx, te_idx in KFold(30).split(X):\n",
    "        ytr = y[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        yte = y[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        Xtr=X[tr_idx[0]:tr_idx[len(tr_idx)-1]+1]\n",
    "        Xte=X[te_idx[0]:te_idx[len(te_idx)-1]+1]\n",
    "        # fit, predict and then get precision, recall, accuracy, F_measure\n",
    "        classifier_method.fit(Xtr.todense(), ytr)\n",
    "        ypre = classifier_method.predict(Xte.todense())\n",
    "        single_precision = precision_score(yte,ypre, average=\"macro\")\n",
    "        single_recall = recall_score(yte,ypre, average=\"macro\")\n",
    "        single_accuracy = accuracy_score(yte,ypre)\n",
    "        single_F_measure = f1_score(yte,ypre, average=\"macro\")\n",
    "        precision.append(single_precision)\n",
    "        recall.append(single_recall)\n",
    "        accuracy.append(single_accuracy)\n",
    "        F_measure.append(single_F_measure)\n",
    "    if count == 0:\n",
    "        print(\"Naïve Bayes\")\n",
    "    elif count == 1:\n",
    "        print(\"Decision Tree\")\n",
    "    else:\n",
    "        print(\"SVM\")\n",
    "    print(\"precision: \", sum(precision)/len(precision))\n",
    "    print(\"recall: \", sum(recall)/len(recall))\n",
    "    print(\"accuracy: \", sum(accuracy)/len(accuracy))\n",
    "    print(\"F_measure: \", sum(F_measure)/len(F_measure))\n",
    "    print(\"\\n\")\n",
    "    count += 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
